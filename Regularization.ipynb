{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "46glAXVan3-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Objective:** Assess understanding of regularization techniques in deep learning. Evaluate application and\n",
        "comparison of different techniques. Enhance knowledge of regularization's role in improving model\n",
        "generalization."
      ],
      "metadata": {
        "id": "Pmvb-98yn35-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is regularization in the context of deep learningH Why is it important ?"
      ],
      "metadata": {
        "id": "177RWo4Fn3xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While in the training of the data, sometimes our model trained too much from the data and become biased to that train data. In such a case model will gives poor accuracy while predicting the outputs for the new data. This problem is called overfitting. And the regularization is the solution of this problem. After using the regularization techniques , our model will save their genralization skills by which our outputs are geranalized and solving the complex problems , even of the new data.These regularization techniques is adding the penality to the cost functions. These are some common regulizers , which are used for reducing the impact of the overfitting.\n",
        "\n",
        "- lasso Regularization (L1)\n",
        "- ridge regularization (L2)\n",
        "- Elasticnet regularization (L1+L2)\n",
        "- Droupout regularization"
      ],
      "metadata": {
        "id": "ZDv_3lVwn3w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff ?"
      ],
      "metadata": {
        "id": "JvHJbZ0vn3rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bias varience Tradeoff :**\n",
        "\n",
        "High bias --------> Underfitting of the model.\n",
        "High Verience --------> Overfitting of the model.\n",
        "\n",
        "A perfect model should -----------> Low bias and Low varience\n",
        "\n",
        "Regularization help to reducing the biases and verience by adding the penality terms to the cost functions. By which the cofficient of the model will become large and able to solve the complex problems without overfitting or underfitting.\n",
        "\n",
        "- Regularization methods, such as L1 and L2 regularization, add a penalty to the loss function based on the complexity of the model. Which is helping to reduce the high bias of the training data.\n",
        "\n",
        "- In the case of L1 regularization, which adds a penalty based on the absolute values of the weights, it encourages sparsity in the model. This means that some weights may be exactly zero, effectively performing feature selection. Feature selection can help address high bias by allowing the model to focus on the most relevant features."
      ],
      "metadata": {
        "id": "LdYR-f_Pn3q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model ?"
      ],
      "metadata": {
        "id": "17E0TIhLn3mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 and L2 both are the regularizations for reducing the impact of overfitting of the model by adding the penality terms to the model. However they both work in a different manner.\n",
        "\n",
        "**L1 Regularization :**\n",
        "\n",
        "In L1 regularization, the penalty term is proportional to the sum of the absolute values of the model parameters (weights).\n",
        "\n",
        "![L1 Regularization.PNG](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAU4AAAAvCAYAAACMjCMiAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAA2MSURBVHhe7Zy/SzPPE8e//5WVhQiC8BTPxyIEHhGUICiCsVCLpDGNKSRFCAiChYKkkCsUQQkEhIA8KTTFISiCIoIiqIVaJNV8Z3b3cnuXveRO4488zgsOnpx3t3u7O++dmd17/gcMwzBMJFg4GYZhIsLCyTAMExEWToZhmIiwcDIMw0SEhZNhGCYiLJwMwzARYeFkGIaJCAsnwzBMRFg4GeYHU39pqH/Rv1/h2f3JtIGFk2F+JA2wt9bBKixCLHcAe7lNKFUrsDE9DoUaq2cnWDgZpkcQHuE7jrp6juAFRXLnHp7LS9A3vQu36vR54Q/M4vnu04D6h+vxZ5QhYeFsR4MGHM++zEfQaBWzttxDaToGfQP/QV98CQq51Q5HHlKJSYgN4fV0Dx6/C2fqWS7H2f8gVX5Vv+5hD8vI1dTPbnJ3ALkPEWSNzyhDESicD/vLMPpLdZQ6+n9Nwmi6DA/qmra82LBXvlE/eozHMqScATd9EO59I1A/tSC1kIejO3XiE/nKshmNu79QSI5D39AkbIQNjVEYZmlcDi3CXoT+ez49gEwcbXloFWx1TnIGG2jT1rX6ieN+fmgFjrE6XffcfopwOtxuzwkBCeO+1+9sON63ILcwA8PUwTlvN/UWr3CU/hjhtHNSlD8mJCJewd6uNMMvnY8vuz3PtV04dAz1x2NDjuxkoQLP6kwn6tU8/KYJfWQdztW5cNwIbzJb0RTxbhemfrnPedhZhH6yWRTQ4v6TOtslfppwUmNGEs6qDVePfyFHndvTwqlE5gOEEx5PwForw9WL+t11yJPwexeKDy+7PZRD+5BQsEeh9ugbSEPpUZ3oCE6KOfRU0b5iaF/hQ33ksQKpzN/mPZTf7Pf9nsjuQrFwYJx038VPFc5ogx1nUhbOr+PagomBAOH8Um7AGos6lv5xTteFBxktAsCJcQTH5gBOQlUnPxkOffuRWExR/2rS6HaMrmDhDEMU4ZRJ8ocL9FZr99iRT3BVO4Hzu9YOfL62obS1CXtl9GpNHlND3ntcPYNbuv3lBo62VjHsuBQDRKxKXp/h3y+boZFcqbyB8+oJesrqpMIsnA24PT2Bo51NsPZN9ZTvQ9ccX2C407jHZ5MX7vyZyrsX9bT1e8V50+F9/jO1074FxZ0K2PR8Py82bCQoN70CR84znEcEle0g6loGa6tseLa/n7DtHi/BxnYz1qMF9JQKc9CP4yJbNr8bEdjHou5uP9XvsB9rvn7U+7Yhrz0+lXWVqPGB9z0YXv9roOgAx9mYu7IdChQJJ99Z6oV8dYCoifHs6SNJXRdw7Gv7OsQE8aOEkww9OSlzook85NJ5sNDA+gbGYeNUXYMD/ihLC1MHKFQoStVVmBiahELNbcw6ztwTQ3NQQDE7Lu+KZPjg9CbY5TwaK3lfN1Bycq9Nb4yMOd1cBPO/Y6twPsHhAl47ZsG5MNQTKKBIjeoh03UZUolxGKT3T69CIb0JxTSGY0NLcIgG/1yzIDX2RwiI7mWIsrD+WWdVNDMHw3hNf9rNf50XMET7tQKH2AbPOJj20lhOUgursC0tvDeFXl3fwAyk1LMs1U5BZRMPlTxMjOWhdHEvBMpeW4Th5K4rXqKf1Hthu+4VlmCD2rp6AFn0ftqHjZRzxbpg+1M7Tyyod9y2tdxe+z6+3V+CqRHZTxm8N7NmQQYFp1/kB/W+zcPezgpktytYN9qXGIN+7MPz2iaGqbtwhGJaymE9hjA8/iaCc7U1KfqruUgTklvKSdIYwffremjdbVpEjVIOi3juBIpJHD9rl+o8UlvF98IJQTgbamIhR6DTZPezPE6JEI6BSSheoAjW1mEqvtQc2KIOarXPQeSGRjbhSvySjTu65a7ii0WtoWU4RMPXVwjlYpcvjMWOCiecMtT0rE6KUMu/hYO2ddD7LKPXh/VHox9NrIPdrMeJaB9XvOh63XDkthB6Z7pf0oDjDAmHM6AQWgX1PEci2zIoVPeXjeA7xFoM9xWOsDwpTC60fYVCRHdSc9oUJwbdQzQR0M5E5z5GyMvC+/twMqnT9pyFSZgquILtLGTO6wsb1bw4p+fyoPEXsv42MHIJ1jSKeTz8MbutCUBYri0Yxfro4zccso/o/Wa3P0cw3oxf1C42YV68L471BPZp9kSeR0S/a4tWcoLwCydGQJ7fSE8JpzEn8lbhNBm7mnH8K4/CINS2DGVQuiHIesfQY1EnFPL8W4UToZBRf2VD2U3h9N/bRLaPew9d79ZJDpRxrI8/PPEPFnM7txdOf9mv6EXj9dpAdZBtpW1XQcSzfdfK60JskQkUzhB9TBjb2kXWwyfgTplV9Vvgb4OvBD3tzAwMkrds6IOOYCSQE/nOb/7Fj0/UbnfWRQQG17swgXbqrvajcxLH99GEVEz2vrYRk6TfvnpHOJ+gpBm8i9mg2xFo7MpY+sZMm353wRZGgoaHg0dspVBIj9PrwRDvFk4UmvP9dbG5eDSxBLnsoiG5H1U4tZyO8P6CQ9+HqgXZJHk3aQzrl3DQtbZzNOG0oUBGa6ir0/ezO64HZ2qTdwtnqD52r2svnGH69rsI5z0cptFLxXrQZOn35MNSx3cUY6Zw1iZd8sUEiNrtNqVNNDt9qUAK38XdlI/guUyLlhgWtnpHOFGwFkzi8EbhNAmNasg+PdQy8LCThsFfM5DZrsDRTl7kOzdO/R5bFOMy1KlxBsUEhq8Udrc1ZiWcge8fYLjKe3BzVk9wmLHUTEtGNo6h6zKUmh6guZ29wkkLInry3V+28vbilhsSK5y+1/f+dVM4aWFALuKF62Onrc1jMUrfBrS/gcifOfom6mDuoZSMuROkSruYvu7pCLbLvJ7rjgpGUW3bvROPT95IwYRR1JSd6P0uoow5z1iqV1a8+09NYTrRM8JJ+T2T2AUYdDsChZO82iT+zWDYdTS8K9GAbqjrDPSggWA0LpUH6ySc4rtev1egCefD/qYMP94knGp/nuerELzOKV/kUn35O72dTzehqOrlFU7tGQJ/2U7uNA/H6oyDXLTw7jHspnDS4oC8J0wfI58unK9wK3ZpRDhC7TCQould1FFt0PJ1Twdosk3gPZ3yy4HgWE1QW4Sptw5OyLQDI7MIw2HqbBQ12Q9TWn62tQ9xfGZxbDbHQBk2tspQSMy0LqZ9J+F0Eu5exSdv4QAylFsxioNsEG+eoj3CIIM6QISvMbGNxeUGrLRjaJQyiMHsGq2kOoMYDc60pUmI36ImBq/YMbQJGY3LkwdTdWoRTu9CkNhErIzwamtFCYESTp/AuLQaLg2YfpHrcd/xGQX9t5P3U8LpMXYV1lMf0Kzs1EsKnkqm00KIx5MziAYJ0pC3bGchjMI/HbE4lPBunQktnGoRRI6lBhxltBCtYx8jzUnKbORyrPomgDbC+Sbv7t3Q2MCJaqRV7KgPaXz5bS0YfFYy3bndO/HGvZvkpNRFn7xVOGW04y6KOYtd2kIQjplM8+8ooms09ug+b+5d8B2EU3yrHpdbV2jgUYcO08qh2hLiHJ7Bh14PrSzKbSHy6Kfv2+PLyhMzgCFKRitHXG/4Hr5OQh2PwehCHsR/YJCkbTnqjwhtV6HtO065zfLjvgGqwu1Ykp6zKrY/laq7wiDFOy5g2b46DY5MKm9O5qT6nW1DmWU09EuwC3RuHOuG91IbaG1E946uuW3k/T8AsB70rpTDEW2Gv5srtKr85sT0CudrczA4NAmpLJadXcYBdQZX+2k898e7ao9eSAHbit4xt7DcNCxj2fJPOOhOYCMxDlMZyiti+47NQGZH7n8VUJto70X9VDzFtoyrLUp0UBto79oKedXYViNpyGWX8PneQd6uj+01/T+skO3keNhkgEV9rGI9Mvtnoq+aY5HaCN/X9rXBcHyzs9F3kas1SregMJjsgSY5qm+y3Dn0FW25DNZFazqqHd4N8Mh7w/R3CSfWBye2UTWmcwuLUKicaPa5AqmM99NhsUOGnAjTvtfvIJzfFRmKeztfJMdxBj/2z+AoBjn0moyehdhQ7TxHbu4OPYDEve8ccG9G1jVMLk14BOrfofmMd6My2tTf1Mf/DgH5OQfPuAxC7oHU9zGH4gWFWdsXfLu/DsX9VZhqipAaW20PX93eKZyS1jHdbgzYuZhMWfk9ZRbOaFBDBuUTjdsWGKaHud1Jw3xkgXiF49y4u1qN3m1xG0Ng8t6c3HLjXnwJ5snZthyXXlvqinBGQHjllHd3wnYNFs6IUOfRKnrNm/uSeVjTfkiG6U0oupqinLb6HQqRhqH0hL7HVW7noc3mEySgb+XThdOGwkgaijvoLfvtmoXzDdB36WtLMJVYhNlpOmZgPnsA9nsT5wzzXSCRGlkCq2LyBL1HaYvy4O5nquLwf2CAcpcT3hsKMkW9veBxCgLSHSycDMN4kZ+Y6p93Rj28uyZQLCsr0E8LUU7Y/pE5zpe/sOcrv+t8RhkKFk6G+aGIRdWkBaWC9YZ9oDdwSLtSUMxpG53YjeH5T1v+bVg4GeYn0/iq3SG9DQsnwzBMRFg4GYZhIsLCyTAMExEWToZhmIiwcDIMw0SEhZNhGCYiLJwMwzARYeFkGIaJCAsnwzBMRFg4GYZhIsLCyTAMExEWToZhmIiwcDIMw0SEhZNhGCYSAP8Hj3jcmiHpDJQAAAAASUVORK5CYII=)\n",
        "\n",
        "\n",
        "This regularization is also called lasso Regularization. This is basically shrinkage the unimportant features of the dataset and make them zero. Which means the feature selection feature is also available in this L1 Regularization technique.It gives the sparse solutions.\n",
        "\n",
        "\n",
        "**L2 Regularization (Ridge) :**\n",
        "\n",
        "In L2 regularization, the penalty term is proportional to the sum of the squared values of the model parameters (weights).\n",
        "\n",
        "![L2 Regularization.PNG](data:image/png;base64,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)\n",
        "\n",
        "As L1 regularization , L2 has no any feature selection ability. It reduces the cofficient but not till zero. It is used for complex problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "sbBRCoL0n3l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models."
      ],
      "metadata": {
        "id": "RiqgmkRdn3hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization techniques are very usefull to preventing the overfitting of the model. Which is accured when our model is too much learned by the training data. The varience is captured too much noise from the data and become biased for the prediction of the new data set. The regularization is adding the penality terms to the cost functions of the models by which the bias and verience of the data has reduced and our model will become gernalize while predicting the new data.Some of the regularization such as L1 has capicity to the feature selection as it make those feature to zero, which has very low significance for the output.Which means noise reduction also done by the regularization which is another cause of reducing the overfitting."
      ],
      "metadata": {
        "id": "rqAg3S5Dn3gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference ?"
      ],
      "metadata": {
        "id": "7eEvOkWan3bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Droupout regularization is also a very usefull regularization technique in the neuoral networks. It working processes is different from the other regularizations. As it randomaly off the x% of neuorones in each itrestion of the training data.And divide the rest neuorones by 1/1-x%.\n",
        "This emphasises on the genralization of model, which reducing the impact of outliers while predicting the outputs. and incresed the model perfromance."
      ],
      "metadata": {
        "id": "p0NrWh0On3bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Describe the concept of Early stopping as a form of regularization. How does it help prevent overfitting during the training process ?"
      ],
      "metadata": {
        "id": "PvbEANgNn3WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of Early stoping , which is genrally a callback function can also helps us to reduce the overfitting of the model.As it stop the traing of the data , when the validation loss doest not improved for a number of epochs.In other words by the Early stoping the training will automatically stoped , when it did'nt find any improvment in the learning process while backward propagation. It will cease to a model to be over trained. And similarly model is escape from the problem of overfitting."
      ],
      "metadata": {
        "id": "JtwRQZ_fn3Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7.Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting ?"
      ],
      "metadata": {
        "id": "kUHV4Q36n3Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization (BN) works by normalizing the inputs of each layer in a neural network during training. It subtracts the mean and divides by the standard deviation, followed by scaling and shifting. This process stabilizes training by reducing internal covariate shift. BN acts as regularization by adding a slight noise during training, preventing overfitting, and allows for faster convergence with larger learning rates. It contributes to improved generalization and reduces sensitivity to weight initialization and hyperparameter choices.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qIAjLeCxn3QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropout ."
      ],
      "metadata": {
        "id": "TVHT5OxZn3LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Regularization"
      ],
      "metadata": {
        "id": "PS_LNfRcjTPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "iTkenvhOh0y6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3iuIAUdnvcN",
        "outputId": "0f5eab1b-8419-49ed-e1e4-ffb8bec15116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# spliting the data into train and test data\n",
        "(x_train_full ,y_train_full),(x_test ,y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_full.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQyFo_D1iAoA",
        "outputId": "546e90ef-05fa-4bf1-e688-e1f92d2eb9ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the validation data\n",
        "x_valid = x_train_full[:5000]\n",
        "y_valid= y_train_full[:5000]"
      ],
      "metadata": {
        "id": "Hfr5W8fLiAmc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_valid.shape)\n",
        "print(y_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uyCkoICiAdI",
        "outputId": "3a0fdd97-da47-42d1-8455-b282e6c51a47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 28, 28)\n",
            "(5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resassigning the train data\n",
        "x_train = x_train_full[5000:]\n",
        "y_train = y_train_full[5000:]"
      ],
      "metadata": {
        "id": "8pEMTCSPiAbQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efpv1MLYiAXy",
        "outputId": "2ba2fd26-7c63-4724-cbeb-66375828316d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55000, 28, 28)\n",
            "(55000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp_b0bWIiAWD",
        "outputId": "4b295550-c213-4c31-835c-e7564c9a5b12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the x data\n",
        "x_train,x_valid, x_test = x_train / 255,x_valid/255 ,x_test / 255"
      ],
      "metadata": {
        "id": "UlDcga5ziZun"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers= [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "]\n"
      ],
      "metadata": {
        "id": "J4R2AqDUiZs0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier= tf.keras.models.Sequential(layers)"
      ],
      "metadata": {
        "id": "pTjBadAwiZoJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.compile(optimizer='SGD', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rLsD9SFyiZmX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.fit(x_train,y_train , epochs=20 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtk8ExYRiZjp",
        "outputId": "ea0228c0-6ff7-49bc-fe51-d6eff4f84012"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1719/1719 [==============================] - 7s 3ms/step - loss: 0.6723 - accuracy: 0.8357 - val_loss: 0.3642 - val_accuracy: 0.9024\n",
            "Epoch 2/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3490 - accuracy: 0.9030 - val_loss: 0.2954 - val_accuracy: 0.9178\n",
            "Epoch 3/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2997 - accuracy: 0.9150 - val_loss: 0.2628 - val_accuracy: 0.9266\n",
            "Epoch 4/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2693 - accuracy: 0.9238 - val_loss: 0.2404 - val_accuracy: 0.9346\n",
            "Epoch 5/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2458 - accuracy: 0.9311 - val_loss: 0.2217 - val_accuracy: 0.9386\n",
            "Epoch 6/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2266 - accuracy: 0.9366 - val_loss: 0.2067 - val_accuracy: 0.9434\n",
            "Epoch 7/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2102 - accuracy: 0.9416 - val_loss: 0.1942 - val_accuracy: 0.9466\n",
            "Epoch 8/20\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1962 - accuracy: 0.9451 - val_loss: 0.1828 - val_accuracy: 0.9494\n",
            "Epoch 9/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1844 - accuracy: 0.9484 - val_loss: 0.1723 - val_accuracy: 0.9530\n",
            "Epoch 10/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1738 - accuracy: 0.9514 - val_loss: 0.1630 - val_accuracy: 0.9556\n",
            "Epoch 11/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1646 - accuracy: 0.9542 - val_loss: 0.1558 - val_accuracy: 0.9574\n",
            "Epoch 12/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1562 - accuracy: 0.9562 - val_loss: 0.1497 - val_accuracy: 0.9596\n",
            "Epoch 13/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1488 - accuracy: 0.9587 - val_loss: 0.1430 - val_accuracy: 0.9620\n",
            "Epoch 14/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1421 - accuracy: 0.9603 - val_loss: 0.1381 - val_accuracy: 0.9620\n",
            "Epoch 15/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1361 - accuracy: 0.9624 - val_loss: 0.1347 - val_accuracy: 0.9640\n",
            "Epoch 16/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1303 - accuracy: 0.9638 - val_loss: 0.1305 - val_accuracy: 0.9656\n",
            "Epoch 17/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1251 - accuracy: 0.9655 - val_loss: 0.1257 - val_accuracy: 0.9652\n",
            "Epoch 18/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1203 - accuracy: 0.9670 - val_loss: 0.1223 - val_accuracy: 0.9662\n",
            "Epoch 19/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1158 - accuracy: 0.9678 - val_loss: 0.1190 - val_accuracy: 0.9668\n",
            "Epoch 20/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1119 - accuracy: 0.9696 - val_loss: 0.1167 - val_accuracy: 0.9664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e97370f2140>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_juro9SiZhz",
        "outputId": "aa8cd0c3-c9b3-4846-bdc6-dfbd96a75fac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9629\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.12146671116352081, 0.9628999829292297]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with Dropout Regularization"
      ],
      "metadata": {
        "id": "eKVCNljajiKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers= [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dropout(0.5), # ---------------------> Dropout layer1\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5), # ---------------------> Dropout layer2\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "]\n"
      ],
      "metadata": {
        "id": "QbY93z1BjeMf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier= tf.keras.models.Sequential(layers)"
      ],
      "metadata": {
        "id": "FDVW2YnikoCs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.compile(optimizer='SGD', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "S6JMmiIHkvIL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.fit(x_train,y_train , epochs=20 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y64rDC4kyb6",
        "outputId": "ae673b0c-5c52-4b5e-d554-70c58e7fdbf7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1719/1719 [==============================] - 8s 3ms/step - loss: 1.1249 - accuracy: 0.6319 - val_loss: 0.4607 - val_accuracy: 0.8852\n",
            "Epoch 2/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6915 - accuracy: 0.7820 - val_loss: 0.3621 - val_accuracy: 0.9078\n",
            "Epoch 3/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6020 - accuracy: 0.8111 - val_loss: 0.3138 - val_accuracy: 0.9168\n",
            "Epoch 4/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5455 - accuracy: 0.8305 - val_loss: 0.2811 - val_accuracy: 0.9278\n",
            "Epoch 5/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5137 - accuracy: 0.8411 - val_loss: 0.2601 - val_accuracy: 0.9334\n",
            "Epoch 6/20\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4826 - accuracy: 0.8502 - val_loss: 0.2459 - val_accuracy: 0.9342\n",
            "Epoch 7/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4594 - accuracy: 0.8584 - val_loss: 0.2315 - val_accuracy: 0.9384\n",
            "Epoch 8/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4441 - accuracy: 0.8641 - val_loss: 0.2181 - val_accuracy: 0.9416\n",
            "Epoch 9/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4218 - accuracy: 0.8691 - val_loss: 0.2092 - val_accuracy: 0.9460\n",
            "Epoch 10/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4077 - accuracy: 0.8746 - val_loss: 0.2011 - val_accuracy: 0.9470\n",
            "Epoch 11/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3985 - accuracy: 0.8744 - val_loss: 0.1946 - val_accuracy: 0.9498\n",
            "Epoch 12/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3858 - accuracy: 0.8805 - val_loss: 0.1846 - val_accuracy: 0.9512\n",
            "Epoch 13/20\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3724 - accuracy: 0.8858 - val_loss: 0.1800 - val_accuracy: 0.9538\n",
            "Epoch 14/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3704 - accuracy: 0.8868 - val_loss: 0.1762 - val_accuracy: 0.9532\n",
            "Epoch 15/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3605 - accuracy: 0.8887 - val_loss: 0.1720 - val_accuracy: 0.9562\n",
            "Epoch 16/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3496 - accuracy: 0.8942 - val_loss: 0.1658 - val_accuracy: 0.9572\n",
            "Epoch 17/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3464 - accuracy: 0.8931 - val_loss: 0.1621 - val_accuracy: 0.9590\n",
            "Epoch 18/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3433 - accuracy: 0.8952 - val_loss: 0.1590 - val_accuracy: 0.9586\n",
            "Epoch 19/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3351 - accuracy: 0.8975 - val_loss: 0.1560 - val_accuracy: 0.9596\n",
            "Epoch 20/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3347 - accuracy: 0.8976 - val_loss: 0.1533 - val_accuracy: 0.9604\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e969aae27a0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2JwUzL_k15L",
        "outputId": "0234d47b-e471-42c1-a92d-e52aaddf8bd6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1539 - accuracy: 0.9581\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1539294272661209, 0.9581000208854675]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- L1 Regularization is prefrable when feature selection is important or when dealing with high-dimensional data where some features may be irrelevant or redundant.As text classification.\n",
        "\n",
        "- L2 is best when we controlling the magnitude of weights and preventing them from becoming too large. Generally suitable for a wide range of problems.And when smoothness is required ..ie..image processing.\n",
        "\n",
        "- combined l1 and l2 , which is elastic net regularization is usefull when we have a highdimensional data. It is best for regression problems.\n",
        "\n",
        "- Dropout will recomended for Neural networks, especially deep architectures, to prevent overfitting and improve generalization...ie..CNN."
      ],
      "metadata": {
        "id": "aivlIvhZl566"
      }
    }
  ]
}